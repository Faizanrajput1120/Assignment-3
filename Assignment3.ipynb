{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to Calculate Metrics\n",
    "1.\tTrain a Classification Model:\n",
    "o\t  Choose a dataset suitable for classification (e.g., Iris dataset, Titanic dataset, etc.).\n",
    "o\t  Split the dataset into training and testing sets.\n",
    "2.\tTrain the Model:\n",
    "o\t    Select a classification algorithm (e.g., Logistic Regression, Decision Trees, Random Forest, etc.).\n",
    "o\t    Train the model on the training set.\n",
    "3.\tMake Predictions:\n",
    "o\t    Use the trained model to predict outcomes on the test set.\n",
    "4.\tCalculate Metrics:\n",
    "o\t    Compare the predicted outcomes with the actual labels from the test set to calculate:\n",
    "\t        Accuracy\n",
    "\t        Precision\n",
    "\t        Recall\n",
    "\t        F1-Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load dataset (Example: Iris dataset)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a classification model (Example: Logistic Regression)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of Results\n",
    "•\tAccuracy: The proportion of correctly classified instances among all instances.\n",
    "•\tPrecision: The proportion of true positive predictions among all positive predictions made.\n",
    "•\tRecall: The proportion of true positive predictions among all actual positive instances.\n",
    "•\tF1-Score: The harmonic mean of precision and recall, providing a balance between the two metrics.\n",
    "Example Output (Hypothetical Values)\n",
    "If you run the above code on the Iris dataset, the output might look like this:\n",
    "Accuracy: 0.9666666666666667\n",
    "Precision: 0.9696969696969697\n",
    "Recall: 0.9666666666666667\n",
    "F1-Score: 0.9665738161559888\n",
    "\n",
    "Classification Report:\n",
    "                        precision    recall    f1-score   support\n",
    "\n",
    "      setosa             1.00         1.00      1.00        10\n",
    "  versicolor             1.00         0.92      0.96        13\n",
    "   virginica             0.89         1.00      0.94         7\n",
    "\n",
    "    accuracy                                    0.97        30\n",
    "   macro avg             0.96        0.97       0.97        30\n",
    "weighted avg             0.97        0.97       0.97        30\n",
    "Explanation\n",
    "•\tThe calculated values (accuracy, precision, recall, and F1-score) are based on the model's predictions compared to the true labels in the test set.\n",
    "•\tThe classification report provides a breakdown of precision, recall, and F1-score for each class in the dataset (setosa, versicolor, virginica), as well as their weighted average across classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Interpretation\n",
    "• Task: Create a confusion matrix for your classification model on the test set.\n",
    "• Question: Present the confusion matrix and explain what each value represents. How\n",
    "does the confusion matrix help in understanding the model&#39;s performance?\n",
    "\n",
    "Creating a confusion matrix for a classification model on the test set is a crucial step in evaluating its performance. Let's go through the process of creating and interpreting a confusion matrix, using our example with the Iris dataset and a Logistic Regression model.\n",
    "Steps to Create and Interpret the Confusion Matrix\n",
    "1.\tTrain a Classification Model:\n",
    "o\tUse the Iris dataset or any suitable dataset for classification.\n",
    "o\tSplit the dataset into training and testing sets.\n",
    "2.\tTrain the Model:\n",
    "o\tChoose a classification algorithm (e.g., Logistic Regression, Decision Trees, SVM).\n",
    "o\tTrain the model on the training set.\n",
    "3.\tMake Predictions:\n",
    "o\tUse the trained model to predict outcomes on the test set.\n",
    "4.\tCreate the Confusion Matrix:\n",
    "o\tCompare the predicted outcomes (y_pred) with the actual labels (y_test) from the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-stdout"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "class_names = iris.target_names\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix (optional but recommended for visualization)\n",
    "plot_confusion_matrix(model, X_test, y_test, display_labels=class_names)\n",
    "plt.title('Confusion Matrix for Iris Classification')\n",
    "plt.show()\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of Confusion Matrix\n",
    "The confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. It helps us understand how well the model is performing in terms of predicting each class.\n",
    "Confusion Matrix Structure\n",
    "In a binary classification case (two classes: positive and negative), the confusion matrix has the following structure:\n",
    "\tPredicted Negative\tPredicted Positive\n",
    "Actual Negative\tTrue Negative (TN)\tFalse Positive (FP)\n",
    "Actual Positive\tFalse Negative (FN)\tTrue Positive (TP)\n",
    "For multi-class classification (like in the Iris dataset with three classes: setosa, versicolor, virginica), the confusion matrix extends to capture the counts of predictions for each class against each actual class.\n",
    "Explanation of Values in the Confusion Matrix\n",
    "•\tTrue Positive (TP): Number of correctly predicted instances of a class (e.g., correctly predicted as setosa).\n",
    "•\tTrue Negative (TN): Number of correctly predicted instances not belonging to a class (e.g., correctly predicted as not setosa for other classes).\n",
    "•\tFalse Positive (FP): Number of incorrectly predicted instances as belonging to a class (e.g., predicted as setosa but actually not setosa).\n",
    "•\tFalse Negative (FN): Number of incorrectly predicted instances as not belonging to a class (e.g., predicted as not setosa but actually setosa).\n",
    "How the Confusion Matrix Helps in Understanding Model Performance\n",
    "•\tAccuracy: Calculate overall accuracy as (TP + TN) / (TP + TN + FP + FN). It gives an overall measure of how often the classifier is correct.\n",
    "•\tPrecision: Calculate precision for each class as TP / (TP + FP). Precision tells us what proportion of positive identifications (in this case, predictions for a specific class) was actually correct.\n",
    "•\tRecall (Sensitivity): Calculate recall for each class as TP / (TP + FN). Recall tells us what proportion of actual positives (in this case, instances of a specific class) was identified correctly by the classifier.\n",
    "•\tF1-Score: Calculate F1-score for each class as 2 * (precision * recall) / (precision + recall). F1-score is the harmonic mean of precision and recall, providing a single metric to evaluate a classifier.\n",
    "By examining the confusion matrix and these associated metrics, we can gain insights into how well the model distinguishes between different classes. It helps us identify which classes are well-predicted and which may need further improvement, guiding adjustments to the model or data preprocessing steps for better performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC/AUC Calculation\n",
    "• Task: Plot the ROC curve and calculate the AUC for your classification model on the test set.\n",
    "• Question: What does the ROC curve look like? What is the AUC value? How do these metrics help in evaluating your model’s performance?\n",
    "Steps to Plot ROC Curve and Calculate AUC\n",
    "1.\tTrain a Classification Model:\n",
    "o\tUse the Iris dataset or any suitable dataset for classification.\n",
    "o\tSplit the dataset into training and testing sets.\n",
    "2.\tTrain the Model:\n",
    "o\tChoose a classification algorithm (e.g., Logistic Regression, Decision Trees, SVM).\n",
    "o\tTrain the model on the training set.\n",
    "3.\tMake Predictions:\n",
    "o\tUse the trained model to predict probabilities of classes on the test set.\n",
    "4.\tPlot ROC Curve and Calculate AUC:\n",
    "o\tCompute the ROC curve using the predicted probabilities and true labels.\n",
    "o\tCalculate the AUC value based on the ROC curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "popout",
     "remove-input",
     "remove-stdout"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get predicted probabilities for ROC curve\n",
    "y_scores = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
    "\n",
    "# Calculate AUC\n",
    "auc = roc_auc_score(y_test, y_scores)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "popout"
    ]
   },
   "source": [
    "## Interpretation of ROC Curve and AUC\n",
    "ROC Curve Interpretation\n",
    "•\tROC Curve: The ROC curve plots the True Positive Rate (Sensitivity) against the False Positive Rate (1 - Specificity) for different thresholds of predicted probabilities.\n",
    "•\tDiagonal Line: Represents random guessing where the True Positive Rate equals the False Positive Rate.\n",
    "AUC (Area Under the Curve)\n",
    "•\tAUC Value: AUC quantifies the overall performance of the classifier across all possible classification thresholds.\n",
    "•\tAUC ranges from 0 to 1, where a higher value indicates better classifier performance.\n",
    "How These Metrics Help in Evaluating Model Performance\n",
    "•\tROC Curve: Provides a visual representation of the trade-off between sensitivity and specificity. A good model will have a curve that is closer to the top-left corner, indicating high true positive rate and low false positive rate across different thresholds.\n",
    "•\tAUC: AUC provides a single scalar value that summarizes the ROC curve. It is particularly useful when you need to compare and select the best model among several. AUC of 0.5 indicates random guessing, while an AUC of 1 indicates perfect classification.\n",
    "Example Output (Hypothetical Values)\n",
    "If you run the above code on the Iris dataset, you would get a plot similar to the ROC curve plot shown above, with different curves for each class (setosa, versicolor, virginica) and corresponding AUC values printed out for each class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation Reporting\n",
    "• Task: Perform k-fold cross-validation (e.g., k=5) for your classification model and report\n",
    "the mean and standard deviation of the accuracy.\n",
    "• Question: What are the mean and standard deviation of the cross-validation accuracy?\n",
    "Why is cross-validation important in model evaluation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing k-fold cross-validation is essential for robustly evaluating the performance of a classification model. Let's walk through how to perform k-fold cross-validation (specifically with k=5) for our example using the Iris dataset and a Logistic Regression model, and then discuss the mean and standard deviation of the cross-validation accuracy.\n",
    "Steps to Perform k-fold Cross-Validation\n",
    "1.\tLoad and Split Data:\n",
    "o\tLoad the Iris dataset and split it into features (X) and target (y).\n",
    "2.\tInitialize the Model:\n",
    "o\tChoose a classification algorithm (e.g., Logistic Regression).\n",
    "3.\tPerform Cross-Validation:\n",
    "o\tUse cross_val_score from scikit-learn to perform k-fold cross-validation.\n",
    "o\tSpecify the model, features (X), target (y), and number of folds (cv).\n",
    "4.\tCalculate Mean and Standard Deviation:\n",
    "o\tCompute the mean and standard deviation of the cross-validation scores to report accuracy.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Perform k-fold cross-validation (Example: k=5)\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\n",
    "\n",
    "# Report mean and standard deviation of accuracy\n",
    "print(\"Cross-validation Accuracy Scores:\", cv_scores)\n",
    "print(\"Mean Accuracy:\", cv_scores.mean())\n",
    "print(\"Standard Deviation of Accuracy:\", cv_scores.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "•\tMean Accuracy: Represents the average accuracy of the model across all folds of cross-validation. It gives us an estimate of how well the model is expected to perform on unseen data.\n",
    "•\tStandard Deviation of Accuracy: Measures the variability or consistency of the model's performance across different folds. A lower standard deviation indicates that the model's performance is more consistent.\n",
    "Why is Cross-Validation Important in Model Evaluation?\n",
    "Cross-validation is crucial for several reasons:\n",
    "1.\tRobustness: It provides a more reliable estimate of model performance compared to a single train-test split. By averaging performance over multiple splits, we reduce the risk of overfitting or underfitting to a particular subset of data.\n",
    "2.\tGeneralization: Cross-validation helps assess how well the model generalizes to unseen data. It gives a more realistic evaluation of the model's ability to perform on new observations by simulating the process of training and testing on multiple subsets of data.\n",
    "3.\tHyperparameter Tuning: Cross-validation is integral in tuning model hyperparameters. It allows us to assess the impact of different hyperparameter values on model performance across multiple validation sets.\n",
    "In summary, cross-validation provides a more comprehensive evaluation of a model's performance by using multiple train-test splits of the data. It yields insights into both the average performance (mean accuracy) and the consistency of performance (standard deviation) across different data subsets, thereby enhancing confidence in the model's capabilities and guiding further improvements or adjustments.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
